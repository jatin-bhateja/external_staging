diff --git a/configure b/configure
old mode 100644
new mode 100755
diff --git a/make/common/JavaCompilation.gmk b/make/common/JavaCompilation.gmk
index cb93131a124..a521f857ea9 100644
--- a/make/common/JavaCompilation.gmk
+++ b/make/common/JavaCompilation.gmk
@@ -264,7 +264,8 @@ define SetupJavaCompilationBody
   endif
 
   # Allow overriding on the command line
-  JAVA_WARNINGS_ARE_ERRORS ?= -Werror
+  # JAVA_WARNINGS_ARE_ERRORS ?= -Werror
+  JAVA_WARNINGS_ARE_ERRORS ?=
 
   # Tell javac to do exactly as told and no more
   PARANOIA_FLAGS := -implicit:none -Xprefer:source -XDignore.symbol.file=true -encoding ascii
diff --git a/src/hotspot/cpu/x86/assembler_x86.cpp b/src/hotspot/cpu/x86/assembler_x86.cpp
index 2119c0797a6..e6b0de7718a 100644
--- a/src/hotspot/cpu/x86/assembler_x86.cpp
+++ b/src/hotspot/cpu/x86/assembler_x86.cpp
@@ -2060,6 +2060,13 @@ void Assembler::vcvtpd2ps(XMMRegister dst, XMMRegister src, int vector_len) {
   emit_int16(0x5A, (0xC0 | encode));
 }
 
+void Assembler::vcvttps2dq(XMMRegister dst, XMMRegister src, int vector_len) {
+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), "");
+  InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);
+  emit_int16(0x5B, (0xC0 | encode));
+}
+
 void Assembler::evcvtqq2ps(XMMRegister dst, XMMRegister src, int vector_len) {
   assert(UseAVX > 2 && VM_Version::supports_avx512dq(), "");
   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
@@ -2068,6 +2075,14 @@ void Assembler::evcvtqq2ps(XMMRegister dst, XMMRegister src, int vector_len) {
   emit_int16(0x5B, (0xC0 | encode));
 }
 
+void Assembler::evcvttpd2qq(XMMRegister dst, XMMRegister src, int vector_len) {
+  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), "");
+  InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
+  attributes.set_is_evex_instruction();
+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);
+  emit_int16(0x7A, (0xC0 | encode));
+}
+
 void Assembler::evcvtqq2pd(XMMRegister dst, XMMRegister src, int vector_len) {
   assert(UseAVX > 2 && VM_Version::supports_avx512dq(), "");
   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
diff --git a/src/hotspot/cpu/x86/assembler_x86.hpp b/src/hotspot/cpu/x86/assembler_x86.hpp
index 09b2a392d30..4f176e3c07a 100644
--- a/src/hotspot/cpu/x86/assembler_x86.hpp
+++ b/src/hotspot/cpu/x86/assembler_x86.hpp
@@ -1168,10 +1168,16 @@ private:
   void vcvtps2pd(XMMRegister dst, XMMRegister src, int vector_len);
   void vcvtpd2ps(XMMRegister dst, XMMRegister src, int vector_len);
 
+  // Convert vector float and int
+  void vcvttps2dq(XMMRegister dst, XMMRegister src, int vector_len);
+
   // Convert vector long to vector FP
   void evcvtqq2ps(XMMRegister dst, XMMRegister src, int vector_len);
   void evcvtqq2pd(XMMRegister dst, XMMRegister src, int vector_len);
 
+  // Convert vector double to long
+  void evcvttpd2qq(XMMRegister dst, XMMRegister src, int vector_len);
+
   // Evex casts with truncation
   void evpmovwb(XMMRegister dst, XMMRegister src, int vector_len);
   void evpmovdw(XMMRegister dst, XMMRegister src, int vector_len);
diff --git a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
index df3501974c4..62faaf65f14 100644
--- a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
@@ -4052,6 +4052,83 @@ void C2_MacroAssembler::masked_op(int ideal_opc, int mask_len, KRegister dst,
   }
 }
 
+void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,
+                                            KRegister ktmp1, AddressLiteral double_sign_flip,
+                                            AddressLiteral max_long, Register scratch, int vec_enc) {
+  Label done;
+  evcvttpd2qq(dst, src, vec_enc);
+  evmovdqul(xtmp1, k0, double_sign_flip, true, vec_enc, scratch);
+  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);
+  kortestwl(ktmp1, ktmp1);
+  jccb(Assembler::equal, done);
+
+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);
+  evcmppd(ktmp1, k0, src, src, Assembler::UNORD_Q, vec_enc);
+  evblendmpd(dst, ktmp1, dst, xtmp2, true, vec_enc);
+
+  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);
+  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_US, vec_enc);
+  evmovdquq(xtmp1, max_long, vec_enc, scratch);
+  evblendmpd(dst, ktmp1, dst, xtmp1, true, vec_enc);
+  bind(done);
+}
+
+void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,
+                                           XMMRegister xtmp2, XMMRegister xtmp3, AddressLiteral float_sign_flip,
+                                           AddressLiteral max_int, Register scratch, int vec_enc) {
+  Label done;
+  // all the special floating point values(nan,posinf,neginf,maxfloat,minfloat)
+  // are converted to 0x80000000.
+  vcvttps2dq(dst, src, vec_enc);
+  vmovdqu(xtmp1, float_sign_flip, scratch);
+
+  // skip special handling if none of the destination lane
+  // contain 0x80000000 value.
+  vpcmpeqd(xtmp2, dst, xtmp1, vec_enc);
+  vpmovmskb(scratch, xtmp2, vec_enc);
+  testl(scratch, scratch);
+  jccb(Assembler::equal, done);
+
+  // handling for special floating value as per
+  // java f2i specifications.
+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);
+  vcmpps(xtmp3, src, src, Assembler::UNORD_Q, vec_enc);
+  vblendvps(dst, dst, xtmp2, xtmp3, vec_enc);
+
+  vpcmpeqd(xtmp1, dst, xtmp1, vec_enc);
+  // mask & src => lane msb 1 for -ve values.
+  vpand(xtmp2, xtmp1, src, vec_enc);
+  // flip msb of mask lanes (msb set for positive special values)
+  vpxor(xtmp3, xtmp2, xtmp1, vec_enc);
+  // exclude non-special value mask lanes.
+  vpand(xtmp3, xtmp3, xtmp1, vec_enc);
+  // replace positive special values (posinf, maxfloat) with maxint.
+  vmovdqu(xtmp1, max_int, scratch);
+  vblendvps(dst, dst, xtmp1, xtmp3, vec_enc);
+  bind(done);
+}
+
+void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,
+                                            KRegister ktmp1, AddressLiteral float_sign_flip,
+                                            AddressLiteral max_int, Register scratch, int vec_enc) {
+  Label done;
+  vcvttps2dq(dst, src, vec_enc);
+  evmovdqul(xtmp1, k0, float_sign_flip, false, vec_enc, scratch);
+  Assembler::evpcmpeqd(ktmp1, k0, xtmp1, dst, vec_enc);
+  kortestwl(ktmp1, ktmp1);
+  jccb(Assembler::equal, done);
+
+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);
+  evcmpps(ktmp1, k0, src, src, Assembler::UNORD_Q, vec_enc);
+  evblendmps(dst, ktmp1, dst, xtmp2, true, vec_enc);
+
+  Assembler::evpcmpeqd(ktmp1, k0, xtmp1, dst, vec_enc);
+  evcmpps(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_US, vec_enc);
+  evmovdquq(xtmp1, max_int, vec_enc, scratch);
+  evblendmps(dst, ktmp1, dst, xtmp1, true, vec_enc);
+  bind(done);
+}
+
 #ifdef _LP64
 void C2_MacroAssembler::vector_mask_operation(int opc, Register dst, KRegister mask,
                                               Register tmp, int masklen, int masksize,
diff --git a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp
index 8efa36a8101..a9437095482 100644
--- a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp
@@ -288,4 +288,16 @@ public:
 
   void masked_op(int ideal_opc, int mask_len, KRegister dst,
                  KRegister src1, KRegister src2);
+
+  void vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,
+                          XMMRegister xtmp2, XMMRegister xtmp3, AddressLiteral float_sign_flip,
+                          AddressLiteral max_int, Register scratch, int vec_enc);
+
+  void vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,
+                           KRegister ktmp1, AddressLiteral float_sign_flip,
+                           AddressLiteral max_int, Register scratch, int vec_enc);
+
+  void vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,
+                           KRegister ktmp1, AddressLiteral double_sign_flip,
+                           AddressLiteral max_long, Register scratch, int vec_enc);
 #endif // CPU_X86_C2_MACROASSEMBLER_X86_HPP
diff --git a/src/hotspot/cpu/x86/x86.ad b/src/hotspot/cpu/x86/x86.ad
index 5a8569dc6e0..de4c4b2cebd 100644
--- a/src/hotspot/cpu/x86/x86.ad
+++ b/src/hotspot/cpu/x86/x86.ad
@@ -1382,6 +1382,10 @@ Assembler::Width widthForType(BasicType bt) {
   static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }
   static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }
   static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }
+  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}
+  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}
+  static address vector_float_signmask() { return StubRoutines::x86::vector_float_sign_mask();}
+  static address vector_double_signmask() { return StubRoutines::x86::vector_double_sign_mask();}
 
 //=============================================================================
 const bool Matcher::match_rule_supported(int opcode) {
@@ -1792,13 +1796,19 @@ const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType
         return false;
       }
       break;
-    case Op_VectorCastF2X:
     case Op_VectorCastD2X:
-      if (is_integral_type(bt)) {
-        // Casts from FP to integral types require special fixup logic not easily
-        // implementable with vectors.
-        return false; // Implementation limitation
+      if (is_subword_type(bt) || bt == T_INT) {
+        return false;
+      }
+      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {
+        return false;
+      }
+      break;
+    case Op_VectorCastF2X:
+      if (is_subword_type(bt) || bt == T_LONG) {
+        return false;
       }
+      break;
     case Op_MulReductionVI:
       if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {
         return false;
@@ -7157,7 +7167,7 @@ instruct vcastLtoX_evex(vec dst, vec src) %{
 instruct vcastFtoD_reg(vec dst, vec src) %{
   predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);
   match(Set dst (VectorCastF2X src));
-  format %{ "vector_cast_f2x  $dst,$src\t!" %}
+  format %{ "vector_cast_f2d  $dst,$src\t!" %}
   ins_encode %{
     int vlen_enc = vector_length_encoding(this);
     __ vcvtps2pd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
@@ -7165,6 +7175,38 @@ instruct vcastFtoD_reg(vec dst, vec src) %{
   ins_pipe( pipe_slow );
 %}
 
+instruct vcastFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP scratch, rFlagsReg cr) %{
+  predicate(!VM_Version::supports_avx512vl() &&
+            Matcher::vector_length_in_bytes(n) < 64 &&
+            Matcher::vector_element_basic_type(n) == T_INT);
+  match(Set dst (VectorCastF2X src));
+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP scratch, KILL cr);
+  format %{ "vector_cast_f2i $dst,$src\t! using $xtmp1, $xtmp2 and $xtmp3 as TEMP" %}
+  ins_encode %{
+    int vlen_enc = vector_length_encoding(this);
+    __ vector_castF2I_avx($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
+                          $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, ExternalAddress(vector_float_signflip()),
+                          ExternalAddress(vector_float_signmask()), $scratch$$Register, vlen_enc);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
+instruct vcastFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, rRegP scratch, rFlagsReg cr) %{
+  predicate((VM_Version::supports_avx512vl() ||
+             Matcher::vector_length_in_bytes(n) == 64) &&
+             Matcher::vector_element_basic_type(n) == T_INT);
+  match(Set dst (VectorCastF2X src));
+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP scratch, KILL cr);
+  format %{ "vector_cast_f2i $dst,$src\t! using $xtmp1, $xtmp2 and $ktmp1 as TEMP" %}
+  ins_encode %{
+    int vlen_enc = vector_length_encoding(this);
+    __ vector_castF2I_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
+                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, ExternalAddress(vector_float_signflip()),
+                           ExternalAddress(vector_float_signmask()), $scratch$$Register, vlen_enc);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
 instruct vcastDtoF_reg(vec dst, vec src) %{
   predicate(Matcher::vector_element_basic_type(n) == T_FLOAT);
   match(Set dst (VectorCastD2X src));
@@ -7176,6 +7218,20 @@ instruct vcastDtoF_reg(vec dst, vec src) %{
   ins_pipe( pipe_slow );
 %}
 
+instruct vcastDtoL_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, rRegP scratch, rFlagsReg cr) %{
+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);
+  match(Set dst (VectorCastD2X src));
+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP scratch, KILL cr);
+  format %{ "vector_cast_d2l $dst,$src\t! using $xtmp1, $xtmp2 and $ktmp1 as TEMP" %}
+  ins_encode %{
+    int vlen_enc = vector_length_encoding(this);
+    __ vector_castD2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,
+                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, ExternalAddress(vector_double_signflip()),
+                           ExternalAddress(vector_double_signmask()), $scratch$$Register, vlen_enc);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
 // --------------------------------- VectorMaskCmp --------------------------------------
 
 instruct vcmpFD(legVec dst, legVec src1, legVec src2, immI8 cond) %{
