-void C2_MacroAssembler::vector_mask_compress(XMMRegister dst, XMMRegister src, Register rtmp1,
-                                             Register rtmp2, XMMRegister xtmp, int mask_len,
-                                             int vec_enc) {
-  int index = 0;
-  int vindex = 0;
-  vpxor(xtmp, xtmp, xtmp, vec_enc);
-  vpsubb(xtmp, xtmp, src, vec_enc);
-  vpmovmskb(rtmp1, xtmp, vec_enc);
-  andq(rtmp1, (0xFFFFFFFFFFFFFFFFUL >> (64 - mask_len)));
-  mov64(rtmp2, -1L);
-  pext(rtmp2, rtmp2, rtmp1);
-  mov64(rtmp1, 0x0101010101010101L);
-  pdep(rtmp1, rtmp2, rtmp1);
-  if (mask_len > 8) {
-    movq(xtmp, rtmp1);
-  }
-  movq(dst, rtmp1);
-
-  mask_len -= 8;
-  while (mask_len > 0) {
-    assert ((mask_len & 0x7) == 0, "mask must be multiple of 8");
-    index++;
-    if ((index % 2) == 0) {
-      pxor(xtmp, xtmp);
-    }
-    mov64(rtmp1, 0x0101010101010101L);
-    shrq(rtmp2, 8);
-    pdep(rtmp1, rtmp2, rtmp1);
-    pinsrq(xtmp, rtmp1, index % 2);
-    vindex = index / 2;
-    if (vindex) {
-      // Write entire 16 byte vector when both 64 bit
-      // lanes are update to save redundant instructions.
-      if (index % 2) {
-        vinsertf128(dst, dst, xtmp, vindex);
-      }
-    } else {
-      vmovdqu(dst, xtmp);
-    }
-    mask_len -= 8;
-  }
-}

