diff --git a/src/hotspot/cpu/x86/assembler_x86.cpp b/src/hotspot/cpu/x86/assembler_x86.cpp
index 66630efa841..ba91fc7ce62 100644
--- a/src/hotspot/cpu/x86/assembler_x86.cpp
+++ b/src/hotspot/cpu/x86/assembler_x86.cpp
@@ -3294,6 +3294,18 @@ void Assembler::evmovdquw(XMMRegister dst, Address src, int vector_len) {
   evmovdquw(dst, k0, src, /*merge*/ false, vector_len);
 }
 
+void Assembler::evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {
+  assert(VM_Version::supports_avx512vlbw(), "");
+  InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ false, /* uses_vl */ true);
+  attributes.set_embedded_opmask_register_specifier(mask);
+  attributes.set_is_evex_instruction();
+  if (merge) {
+    attributes.reset_is_clear_context();
+  }
+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);
+  emit_int16(0x7F, (0xC0 | encode));
+}
+
 void Assembler::evmovdquw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {
   assert(VM_Version::supports_avx512vlbw(), "");
   InstructionMark im(this);
@@ -13571,7 +13583,7 @@ void Assembler::notq(Register dst) {
   emit_int16((unsigned char)0xF7, (0xD0 | encode));
 }
 
-void Assembler::bt(Register dst, Register src) {
+void Assembler::btq(Register dst, Register src) {
   int encode = prefixq_and_encode(src->encoding(), dst->encoding());
   emit_int24(0x0F, (unsigned char)0xA3, (encode | 0xC0));
 }
diff --git a/src/hotspot/cpu/x86/assembler_x86.hpp b/src/hotspot/cpu/x86/assembler_x86.hpp
index 31a2b047ff5..1d6ad8c474b 100644
--- a/src/hotspot/cpu/x86/assembler_x86.hpp
+++ b/src/hotspot/cpu/x86/assembler_x86.hpp
@@ -1735,7 +1735,7 @@ class Assembler : public AbstractAssembler  {
   void btsq(Address dst, int imm8);
   void btrq(Address dst, int imm8);
 #endif
-  void bt(Register dst, Register src);
+  void btq(Register dst, Register src);
 
   void orw(Register dst, Register src);
 
diff --git a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
index ff920c19ddf..22467f1278c 100644
--- a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
@@ -1571,6 +1571,164 @@ void C2_MacroAssembler::vinsert(BasicType typ, XMMRegister dst, XMMRegister src,
   }
 }
 
+void C2_MacroAssembler::vpackI2X(BasicType elem_bt, XMMRegister dst, int vlen_enc) {
+  assert(VM_Version::supports_avx512vl(), "");
+  if (elem_bt == T_SHORT) {
+    evpmovdw(dst, dst, vlen_enc);
+  } else {
+    assert(elem_bt == T_BYTE, "");
+    evpmovdb(dst, dst, vlen_enc);
+  }
+}
+
+void C2_MacroAssembler::vgather_subword_avx3(BasicType elem_bt, XMMRegister dst, Register base, XMMRegister offset,
+                                             Register idx_base, int idx_off, XMMRegister idx_vec, XMMRegister ones,
+                                             XMMRegister xtmp, KRegister gmask, int vlen_enc) {
+  assert(is_subword_type(elem_bt), "");
+  int normalize_index_shift = elem_bt == T_BYTE ? 2 : 1;
+  int offset_compute_mask = elem_bt == T_BYTE ? 30 : 31;
+  int offset_compute_shift = elem_bt == T_BYTE ? 3 : 4;
+  Address::ScaleFactor scale = elem_bt == T_BYTE ? Address::times_1 : Address::times_2;
+  evmovdquq(idx_vec, Address(idx_base, idx_off, Address::times_4), vlen_enc);
+  if (offset != xnoreg) {
+    vpaddd(idx_vec, idx_vec, offset, vlen_enc);
+  }
+  // Normalize the indices to multiple of 2.
+  vpslld(xtmp, ones, normalize_index_shift, vlen_enc);
+  vpand(xtmp, idx_vec, xtmp, vlen_enc);
+  // Load double words from normalized indices.
+  evpgatherdd(dst, gmask, Address(base, xtmp, scale), vlen_enc);
+  // Compute bit level offset of actual short value within each doubleword
+  // lane.
+  vpsrld(xtmp, ones, offset_compute_mask, vlen_enc);
+  vpand(xtmp, idx_vec, xtmp, vlen_enc);
+  vpslld(xtmp, xtmp, offset_compute_shift, vlen_enc);
+  // Move the sub-word value at respective bit offset to lower order bits of each
+  // double word lane.
+  vpsrlvd(dst, dst, xtmp, vlen_enc);
+  // Pack double word vector into sub-word vector.
+  vpackI2X(elem_bt, dst, vlen_enc);
+}
+
+void C2_MacroAssembler::vgather_subword_masked_avx3(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,
+                                                    Register offset, XMMRegister offset_vec, XMMRegister idx_vec,
+                                                    XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, KRegister mask,
+                                                    KRegister gmask, int vlen_enc, int vlen) {
+  int shuf_mask[] = {0xFC, 0xF3, 0xCF, 0x3F};
+  int lane_count_subwords = vlen;
+  int lane_count_ints = MIN2(Matcher::max_vector_size(T_INT), vlen);
+  Assembler::AvxVectorLen int_vec_enc =
+      vector_length_encoding(lane_count_ints * type2aelembytes(T_INT));
+
+  if (offset != noreg) {
+    evpbroadcastd(offset_vec, offset, int_vec_enc);
+  }
+  vpxor(dst, dst, dst, int_vec_enc);
+  vallones(xtmp1, int_vec_enc);
+
+  if (elem_bt == T_BYTE) {
+    // Loop to gather 8(64bit), 16(128bit), 32(256bit) or 64(512bit) bytes from
+    // memory into vector using integral gather instructions. Number of loop
+    // iterations depends on the maximum integral vector size supported by
+    // target capped by the gather count i.e. in order to gather 8 bytes over
+    // AVX-512 targets we need to use 256bit integer gather even though target
+    // supports 512 bit integral gather operation.
+    for (int i = 0, j = 0; i < lane_count_subwords; i += lane_count_ints, j++) {
+      vpxor(xtmp2, xtmp2, xtmp2, int_vec_enc);
+      kxnorwl(gmask, gmask, gmask);
+      vgather_subword_avx3(elem_bt, xtmp2, base, offset_vec, idx_base, i,
+                           idx_vec, xtmp1, xtmp3, gmask, int_vec_enc);
+      if (vlen_enc == Assembler::AVX_512bit) {
+        // Case to handle 64 byte gather operation.
+        assert(int_vec_enc == Assembler::AVX_512bit, "");
+        // Appropriately permute 128 bit lane holding 16 bytes accumulated using
+        // 512 bit integral gather operation.
+        if (j > 0) {
+          vinserti32x4(dst, dst, xtmp2, j);
+        } else {
+          evmovdquq(dst, xtmp2, vlen_enc);
+        }
+      } else if (vlen_enc == Assembler::AVX_256bit) {
+        // Case to handle 32 byte gather operation.
+        if (j > 0) {
+          if (int_vec_enc == Assembler::AVX_512bit) {
+            vinserti32x4(dst, dst, xtmp2, j);
+          } else {
+            assert(int_vec_enc == Assembler::AVX_256bit, "");
+            // Permute 8 bytes loaded using 256 bit integral gather.
+            vpermq(xtmp2, xtmp2, shuf_mask[j], vlen_enc);
+            vpor(dst, dst, xtmp2, vlen_enc);
+          }
+        } else {
+          vpor(dst, dst, xtmp2, vlen_enc);
+        }
+      } else if (vlen_enc == Assembler::AVX_128bit) {
+        // Case to handle 16 byte gather operation.
+        if (j > 0) {
+          // We enter here only if maximum integer vector size is less than 512
+          // bits.
+          if (int_vec_enc == Assembler::AVX_256bit) {
+            vpermq(xtmp2, xtmp2, shuf_mask[j], int_vec_enc);
+          } else {
+            assert(int_vec_enc == Assembler::AVX_128bit, "");
+            vpshufd(xtmp2, xtmp2, shuf_mask[j], vlen_enc);
+          }
+        }
+        vpor(dst, dst, xtmp2, vlen_enc);
+      }
+    }
+  } else {
+    assert(elem_bt == T_SHORT, "");
+    // Loop to gather 4(64bit), 8(128bit), 16(256bit) or 32(512bit) short values
+    // from memory into vector using integral gather instruction.
+    for (int i = 0, j = 0; i < lane_count_subwords; i += lane_count_ints, j++) {
+      vpxor(xtmp2, xtmp2, xtmp2, int_vec_enc);
+      kxnorwl(gmask, gmask, gmask);
+      vgather_subword_avx3(elem_bt, xtmp2, base, offset_vec, idx_base, i,
+                           idx_vec, xtmp1, xtmp3, gmask, int_vec_enc);
+      if (vlen_enc == Assembler::AVX_512bit) {
+        // Case to handle 32 byte gather operation.
+        assert(int_vec_enc == Assembler::AVX_512bit, "");
+        // Appropriately permute 256 bit lane holding 16 shorts accumulated
+        // using 512 bit integral gather operation.
+        if (j > 0) {
+          vinserti64x4(dst, dst, xtmp2, j);
+        } else {
+          evmovdquq(dst, xtmp2, vlen_enc);
+        }
+      } else if (vlen_enc == Assembler::AVX_256bit) {
+        // Case to handle 16 byte gather operation.
+        if (int_vec_enc == Assembler::AVX_512bit) {
+          // All 16 short values are loaded in one short by 512 bit integral
+          // gather.
+          vmovdqu(dst, xtmp2);
+        } else {
+          assert(int_vec_enc == Assembler::AVX_256bit, "");
+          // Permute 8 short values loaded using 256 bit integral gather.
+          vinserti32x4(dst, dst, xtmp2, j);
+        }
+      } else if (vlen_enc == Assembler::AVX_128bit) {
+        if (j > 0) {
+          // We enter here only if maximum integer vector size is less than 256
+          // bits.
+          assert(int_vec_enc == Assembler::AVX_128bit, "");
+          vpslldq(xtmp2, xtmp2, 8, vlen_enc);
+        }
+        vpor(dst, dst, xtmp2, vlen_enc);
+      }
+    }
+  }
+
+  if (mask != knoreg) {
+    if (elem_bt == T_BYTE) {
+      evmovdqub(dst, mask, dst, false, vlen_enc);
+    } else {
+      assert(elem_bt == T_SHORT, "");
+      evmovdquw(dst, mask, dst, false, vlen_enc);
+    }
+  }
+}
+
 #ifdef _LP64
 void C2_MacroAssembler::vgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,
                                           Register mask, Register midx, Register rtmp, int vlen_enc) {
@@ -1579,7 +1737,8 @@ void C2_MacroAssembler::vgather8b_masked(BasicType elem_bt, XMMRegister dst, Reg
     Label case0, case1, case2, case3;
     Label* larr[] = { &case0, &case1, &case2, &case3 };
     for (int i = 0; i < 4; i++) {
-      bt(mask, midx);
+      // dst[i] = mask ? src[index[i]] : 0
+      btq(mask, midx);
       jccb(Assembler::carryClear, *larr[i]);
       movl(rtmp, Address(idx_base, i*4));
       pinsrw(dst, Address(base, rtmp, Address::times_2), i);
@@ -1591,7 +1750,8 @@ void C2_MacroAssembler::vgather8b_masked(BasicType elem_bt, XMMRegister dst, Reg
     Label case0, case1, case2, case3, case4, case5, case6, case7;
     Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };
     for (int i = 0; i < 8; i++) {
-      bt(mask, midx);
+      // dst[i] = mask ? src[index[i]] : 0
+      btq(mask, midx);
       jccb(Assembler::carryClear, *larr[i]);
       movl(rtmp, Address(idx_base, i*4));
       pinsrb(dst, Address(base, rtmp), i);
@@ -1608,7 +1768,8 @@ void C2_MacroAssembler::vgather8b_masked_offset(BasicType elem_bt, XMMRegister d
     Label case0, case1, case2, case3;
     Label* larr[] = { &case0, &case1, &case2, &case3 };
     for (int i = 0; i < 4; i++) {
-      bt(mask, midx);
+      // dst[i] = mask ? src[offset + index[i]] : 0
+      btq(mask, midx);
       jccb(Assembler::carryClear, *larr[i]);
       movl(rtmp, Address(idx_base, i*4));
       addl(rtmp, offset);
@@ -1621,7 +1782,8 @@ void C2_MacroAssembler::vgather8b_masked_offset(BasicType elem_bt, XMMRegister d
     Label case0, case1, case2, case3, case4, case5, case6, case7;
     Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };
     for (int i = 0; i < 8; i++) {
-      bt(mask, midx);
+      // dst[i] = mask ? src[offset + index[i]] : 0
+      btq(mask, midx);
       jccb(Assembler::carryClear, *larr[i]);
       movl(rtmp, Address(idx_base, i*4));
       addl(rtmp, offset);
@@ -1637,31 +1799,37 @@ void C2_MacroAssembler::vgather8b(BasicType elem_bt, XMMRegister dst, Register b
   vpxor(dst, dst, dst, vlen_enc);
   if (elem_bt == T_SHORT) {
     for (int i = 0; i < 4; i++) {
-      movl(rtmp, Address(idx_base, i*4));
+      // dst[i] = src[index[i]]
+      movl(rtmp, Address(idx_base, i * 4));
       pinsrw(dst, Address(base, rtmp, Address::times_2), i);
     }
   } else {
     assert(elem_bt == T_BYTE, "");
     for (int i = 0; i < 8; i++) {
-      movl(rtmp, Address(idx_base, i*4));
+      // dst[i] = src[index[i]]
+      movl(rtmp, Address(idx_base, i * 4));
       pinsrb(dst, Address(base, rtmp), i);
     }
   }
 }
 
-void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,
-                                          Register offset, Register rtmp, int vlen_enc) {
+void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst,
+                                         Register base, Register idx_base,
+                                         Register offset, Register rtmp,
+                                         int vlen_enc) {
   vpxor(dst, dst, dst, vlen_enc);
   if (elem_bt == T_SHORT) {
     for (int i = 0; i < 4; i++) {
-      movl(rtmp, Address(idx_base, i*4));
+      // dst[i] = src[offset + index[i]]
+      movl(rtmp, Address(idx_base, i * 4));
       addl(rtmp, offset);
       pinsrw(dst, Address(base, rtmp, Address::times_2), i);
     }
   } else {
     assert(elem_bt == T_BYTE, "");
     for (int i = 0; i < 8; i++) {
-      movl(rtmp, Address(idx_base, i*4));
+      // dst[i] = src[offset + index[i]]
+      movl(rtmp, Address(idx_base, i * 4));
       addl(rtmp, offset);
       pinsrb(dst, Address(base, rtmp), i);
     }
@@ -1676,7 +1844,7 @@ void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst, Reg
  *
  * DST_VEC = ZERO_VEC
  * PERM_INDEX = {0, 1, 2, 3, 4, 5, 6, 7, 8..}
- * TWO_VEC = {2, 2, 2, 2, 2, 2, 2, 2..}
+ * TWO_VEC    = {2, 2, 2, 2, 2, 2, 2, 2..}
  * FOREACH_ITER:
  *     TMP_VEC_64 = PICK_SUB_WORDS_FROM_GATHER_INDICES
  *     TEMP_PERM_VEC = PERMUTE TMP_VEC_64 PERM_INDEX
diff --git a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp
index 5fa44a4b634..f73f8ae586b 100644
--- a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.hpp
@@ -496,6 +496,15 @@
   void vector_rearrange_int_float(BasicType bt, XMMRegister dst, XMMRegister shuffle,
                                   XMMRegister src, int vlen_enc);
 
+  void vpackI2X(BasicType elem_bt, XMMRegister dst, int vlen_enc);
+
+  void vgather_subword_avx3(BasicType elem_bt, XMMRegister dst, Register base, XMMRegister offset,
+                            Register idx_base, int idx_off, XMMRegister idx_vec, XMMRegister ones,
+                            XMMRegister xtmp, KRegister gmask, int vlen_enc);
+
+  void vgather_subword_masked_avx3(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,
+                                   Register offset, XMMRegister offset_vec, XMMRegister idx_vec, XMMRegister xtmp1,
+                                   XMMRegister xtmp2, XMMRegister xtmp3, KRegister mask, KRegister ktmp, int vlen_enc, int vlen);
 
   void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,
                        Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,
diff --git a/src/hotspot/cpu/x86/matcher_x86.hpp b/src/hotspot/cpu/x86/matcher_x86.hpp
index 192e959451f..bee538e16db 100644
--- a/src/hotspot/cpu/x86/matcher_x86.hpp
+++ b/src/hotspot/cpu/x86/matcher_x86.hpp
@@ -226,7 +226,7 @@
         return VM_Version::supports_avx512vldq() ? 0 : 6;
       case Op_LoadVectorGather:
       case Op_LoadVectorGatherMasked:
-        return is_subword_type(ety) ? 50 : 0;
+        return is_subword_type(ety) ? 70 : 0;
       case Op_VectorCastF2X: // fall through
       case Op_VectorCastD2X:
         return is_floating_point_type(ety) ? 0 : (is_subword_type(ety) ? 35 : 30);
diff --git a/src/hotspot/cpu/x86/x86.ad b/src/hotspot/cpu/x86/x86.ad
index 67ab1e369b6..17169534655 100644
--- a/src/hotspot/cpu/x86/x86.ad
+++ b/src/hotspot/cpu/x86/x86.ad
@@ -4108,150 +4108,157 @@ instruct evgather_masked(vec dst, memory mem, vec idx, kReg mask, kReg ktmp, rRe
   ins_pipe( pipe_slow );
 %}
 
-instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{
-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
+instruct vgather_subword_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && VM_Version::supports_avx512vl());
   match(Set dst (LoadVectorGather mem (Binary idx offset)));
-  effect(TEMP tmp, TEMP rtmp);
-  format %{ "vector_gatherLE8 $dst, $mem, $idx\t! using $tmp and $rtmp as TEMP" %}
+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);
+  format %{ "vector_gather_subword $dst, $mem, $idx\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2 and  $xtmp3 as TEMP" %}
   ins_encode %{
+    uint vlen = Matcher::vector_length(this);
     int vlen_enc = vector_length_encoding(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
     __ lea($tmp$$Register, $mem$$Address);
-    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);
+    __ movptr($idx_base_temp$$Register, $idx$$Register);
+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, xnoreg, $idx_vec$$XMMRegister,
+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, knoreg, $ktmp$$KRegister, vlen_enc, vlen);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,
-                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{
-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
+instruct vgather_subword_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec offset_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && VM_Version::supports_avx512vl());
   match(Set dst (LoadVectorGather mem (Binary idx offset)));
-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);
-  format %{ "vector_gatherGT8 $dst, $mem, $idx\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP" %}
+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP offset_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);
+  format %{ "vector_gather_subword $dst, $mem, $idx, $offset\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP" %}
   ins_encode %{
+    uint vlen = Matcher::vector_length(this);
     int vlen_enc = vector_length_encoding(this);
-    int vector_len = Matcher::vector_length(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
-    __ lea($tmp$$Register, $mem$$Address);
     __ movptr($idx_base_temp$$Register, $idx$$Register);
-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,
-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);
+    __ lea($tmp$$Register, $mem$$Address);
+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $offset_vec$$XMMRegister, $idx_vec$$XMMRegister,
+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, knoreg, $ktmp$$KRegister, vlen_enc, vlen);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{
-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
-  match(Set dst (LoadVectorGather mem (Binary idx offset)));
-  ins_cost(200);
-  effect(TEMP tmp, TEMP rtmp, KILL cr);
-  format %{ "vector_gatherLE8 $dst, $mem, $idx, $offset\t! using $tmp and $rtmp as TEMP" %}
+instruct vgather_subword_mask_avx3(vec dst, memory mem, rRegP idx, kReg mask, immI_0 offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && n->in(MemNode::ValueIn)->in(2)->in(1)->bottom_type()->isa_vectmask());
+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);
+  format %{ "vector_gather_subword $dst, $mem, $idx, $offset\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP" %}
   ins_encode %{
+    uint vlen = Matcher::vector_length(this);
     int vlen_enc = vector_length_encoding(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
     __ lea($tmp$$Register, $mem$$Address);
-    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);
+    __ movptr($idx_base_temp$$Register, $idx$$Register);
+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, xnoreg, $idx_vec$$XMMRegister,
+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $mask$$KRegister, $ktmp$$KRegister, vlen_enc, vlen);
   %}
   ins_pipe( pipe_slow );
 %}
 
+instruct vgather_subword_mask_off_avx3(vec dst, memory mem, rRegP idx, kReg mask, rRegI offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec offset_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && n->in(MemNode::ValueIn)->in(2)->in(1)->bottom_type()->isa_vectmask());
+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP offset_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);
+  format %{ "vector_gather_subword $dst, $mem, $idx, $offset, $mask\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP" %}
+  ins_encode %{
+    uint vlen = Matcher::vector_length(this);
+    int vlen_enc = vector_length_encoding(this);
+    BasicType elem_bt = Matcher::vector_element_basic_type(this);
+    __ movptr($idx_base_temp$$Register, $idx$$Register);
+    __ lea($tmp$$Register, $mem$$Address);
+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $offset_vec$$XMMRegister, $idx_vec$$XMMRegister,
+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $mask$$KRegister, $ktmp$$KRegister, vlen_enc, vlen);
+  %}
+  ins_pipe( pipe_slow );
+%}
 
-instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,
-                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{
-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
+instruct vgather_subwordLE8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 8);
   match(Set dst (LoadVectorGather mem (Binary idx offset)));
-  ins_cost(200);
-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);
-  format %{ "vector_gatherGT8 $dst, $mem, $idx, $offset\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP" %}
+  effect(TEMP tmp, TEMP rtmp);
+  format %{ "vector_gatherLE8 $dst, $mem, $idx\t! using $tmp and $rtmp as TEMP" %}
   ins_encode %{
     int vlen_enc = vector_length_encoding(this);
-    int vector_len = Matcher::vector_length(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
     __ lea($tmp$$Register, $mem$$Address);
-    __ movptr($idx_base_temp$$Register, $idx$$Register);
-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,
-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);
+    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
-
-#ifdef _LP64
-instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{
-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
-  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);
-  format %{ "vector_masked_gatherLE8 $dst, $mem, $idx, $mask\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP" %}
+instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
+  match(Set dst (LoadVectorGather mem (Binary idx offset)));
+  effect(TEMP tmp, TEMP rtmp);
+  format %{ "vector_gatherLE8 $dst, $mem, $idx\t! using $tmp and $rtmp as TEMP" %}
   ins_encode %{
     int vlen_enc = vector_length_encoding(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
-    __ xorq($midx$$Register, $midx$$Register);
     __ lea($tmp$$Register, $mem$$Address);
-    __ kmovql($rtmp2$$Register, $mask$$KRegister);
-    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);
+    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegP tmp, rRegP idx_base_temp,
-                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length, rFlagsReg cr) %{
-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
-  ins_cost(200);
-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length, KILL cr);
-  format %{ "vector_gatherGT8_masked $dst, $mem, $idx, $mask\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP" %}
+instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,
+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) > 8);
+  match(Set dst (LoadVectorGather mem (Binary idx offset)));
+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);
+  format %{ "vector_gatherGT8 $dst, $mem, $idx\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP" %}
   ins_encode %{
     int vlen_enc = vector_length_encoding(this);
     int vector_len = Matcher::vector_length(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
-    __ xorq($midx$$Register, $midx$$Register);
     __ lea($tmp$$Register, $mem$$Address);
     __ movptr($idx_base_temp$$Register, $idx$$Register);
-    __ kmovql($rtmp2$$Register, $mask$$KRegister);
-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,
-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);
+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,
+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vgather_masked_subwordLE8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{
-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
-  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);
-  format %{ "vector_masked_gatherLE8_off $dst, $mem, $idx, $offset, $mask\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP" %}
+instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 8);
+  match(Set dst (LoadVectorGather mem (Binary idx offset)));
+  ins_cost(200);
+  effect(TEMP tmp, TEMP rtmp, KILL cr);
+  format %{ "vector_gatherLE8 $dst, $mem, $idx, $offset\t! using $tmp and $rtmp as TEMP" %}
   ins_encode %{
     int vlen_enc = vector_length_encoding(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
-    __ xorq($midx$$Register, $midx$$Register);
     __ lea($tmp$$Register, $mem$$Address);
-    __ kmovql($rtmp2$$Register, $mask$$KRegister);
-    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register,
-                                $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);
+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vgather_masked_subwordGT8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegP tmp, rRegP idx_base_temp,
-                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length, rFlagsReg cr) %{
-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);
-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
+
+instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,
+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{
+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) > 8);
+  match(Set dst (LoadVectorGather mem (Binary idx offset)));
   ins_cost(200);
-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length, KILL cr);
-  format %{ "vector_gatherGT8_masked_off $dst, $mem, $idx, $offset, $mask\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP" %}
+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);
+  format %{ "vector_gatherGT8 $dst, $mem, $idx, $offset\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP" %}
   ins_encode %{
     int vlen_enc = vector_length_encoding(this);
     int vector_len = Matcher::vector_length(this);
     BasicType elem_bt = Matcher::vector_element_basic_type(this);
-    __ xorq($midx$$Register, $midx$$Register);
     __ lea($tmp$$Register, $mem$$Address);
     __ movptr($idx_base_temp$$Register, $idx$$Register);
-    __ kmovql($rtmp2$$Register, $mask$$KRegister);
-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,
-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);
+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,
+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
+
+#ifdef _LP64
 instruct vgather_masked_subwordLE8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, vec mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{
   predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);
   match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));
